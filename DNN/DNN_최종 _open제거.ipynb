{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import f_oneway\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "past_sales = pd.read_csv('C:/Users/PC/표/수요예측/rossmann-store-sales/train.csv')\n",
    "store = pd.read_csv('C:/Users/PC/표/수요예측/rossmann-store-sales/store.csv')\n",
    "\n",
    "# 데이터 제거\n",
    "store = store.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval'], axis=1)\n",
    "\n",
    "# 데이터 정제 : 날짜\n",
    "past_sales['Date'] = pd.to_datetime(past_sales['Date'], format=\"%Y-%m-%d\")\n",
    "past_sales['Month']=pd.DatetimeIndex(past_sales.Date).month\n",
    "past_sales['Day']=pd.DatetimeIndex(past_sales.Date).day\n",
    "past_sales = past_sales.drop(['Date'],axis=1)\n",
    "\n",
    "# 데이터 정제 : StateHoliday\n",
    "past_sales['StateHoliday'] = past_sales['StateHoliday'].replace({0:'d'}) # 0 값은 d로 대체\n",
    "past_sales['StateHoliday'] = past_sales['StateHoliday'].replace({'0':'d'})# 0 값은 d로 대체\n",
    "\n",
    "# 데이터 병합\n",
    "data = pd.merge(left = past_sales,right = store, on = 'Store', how = 'right')\n",
    "\n",
    "# 결측값 처리\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# store 이름 제거\n",
    "data = data.drop(['Store'],axis=1)\n",
    "\n",
    "# 데이터 분리(수치형, 범주형)\n",
    "digital_cols = ['Sales', 'Customers', 'CompetitionDistance', 'Month', 'Day']\n",
    "digital_data = data[digital_cols]\n",
    "digital_data = pd.DataFrame(digital_data.values, columns = digital_cols)\n",
    "\n",
    "categorey_clos = data.columns.drop(digital_cols)\n",
    "categorey_data = data[categorey_clos]\n",
    "categorey_data = pd.DataFrame(categorey_data.values, columns = categorey_clos)\n",
    "\n",
    "# 인코딩\n",
    "onehotencoding_cols = ['StateHoliday', 'StoreType', 'Assortment', 'DayOfWeek']\n",
    "categorey_data = pd.get_dummies(data = categorey_data, columns = onehotencoding_cols)\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler().fit(digital_data)\n",
    "digital_data = pd.DataFrame(scaler.transform(digital_data),index = digital_data.index, columns = digital_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 병합\n",
    "data = pd.concat([digital_data, categorey_data], axis = 1)\n",
    "\n",
    "# 불필요한 데이터 제거\n",
    "data = data.drop(['StateHoliday_d', 'StoreType_a', 'Assortment_a', 'DayOfWeek_4', 'DayOfWeek_7', 'Open'],axis=1)\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>StateHoliday_a</th>\n",
       "      <th>StateHoliday_b</th>\n",
       "      <th>StateHoliday_c</th>\n",
       "      <th>StoreType_b</th>\n",
       "      <th>StoreType_c</th>\n",
       "      <th>StoreType_d</th>\n",
       "      <th>Assortment_b</th>\n",
       "      <th>Assortment_c</th>\n",
       "      <th>DayOfWeek_1</th>\n",
       "      <th>DayOfWeek_2</th>\n",
       "      <th>DayOfWeek_3</th>\n",
       "      <th>DayOfWeek_5</th>\n",
       "      <th>DayOfWeek_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.133481</td>\n",
       "      <td>-0.169168</td>\n",
       "      <td>-0.539198</td>\n",
       "      <td>0.346614</td>\n",
       "      <td>1.740763</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.196581</td>\n",
       "      <td>-0.188537</td>\n",
       "      <td>-0.539198</td>\n",
       "      <td>0.346614</td>\n",
       "      <td>1.626967</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.258383</td>\n",
       "      <td>-0.238037</td>\n",
       "      <td>-0.539198</td>\n",
       "      <td>0.346614</td>\n",
       "      <td>1.513171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.198918</td>\n",
       "      <td>-0.158407</td>\n",
       "      <td>-0.539198</td>\n",
       "      <td>0.346614</td>\n",
       "      <td>1.399374</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.084382</td>\n",
       "      <td>-0.046494</td>\n",
       "      <td>-0.539198</td>\n",
       "      <td>0.346614</td>\n",
       "      <td>1.285578</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014562</th>\n",
       "      <td>-0.261239</td>\n",
       "      <td>-0.634036</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-1.457264</td>\n",
       "      <td>-1.217938</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014563</th>\n",
       "      <td>-0.321223</td>\n",
       "      <td>-0.662014</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-1.457264</td>\n",
       "      <td>-1.331734</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014564</th>\n",
       "      <td>-0.384323</td>\n",
       "      <td>-0.717970</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-1.457264</td>\n",
       "      <td>-1.445531</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014565</th>\n",
       "      <td>-0.540124</td>\n",
       "      <td>-0.707209</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-1.457264</td>\n",
       "      <td>-1.559327</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014566</th>\n",
       "      <td>-1.500124</td>\n",
       "      <td>-1.363620</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-1.457264</td>\n",
       "      <td>-1.673123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014567 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sales  Customers  CompetitionDistance     Month       Day Promo  \\\n",
       "0       -0.133481  -0.169168            -0.539198  0.346614  1.740763     1   \n",
       "1       -0.196581  -0.188537            -0.539198  0.346614  1.626967     1   \n",
       "2       -0.258383  -0.238037            -0.539198  0.346614  1.513171     1   \n",
       "3       -0.198918  -0.158407            -0.539198  0.346614  1.399374     1   \n",
       "4        0.084382  -0.046494            -0.539198  0.346614  1.285578     1   \n",
       "...           ...        ...                  ...       ...       ...   ...   \n",
       "1014562 -0.261239  -0.634036            -0.010380 -1.457264 -1.217938     0   \n",
       "1014563 -0.321223  -0.662014            -0.010380 -1.457264 -1.331734     0   \n",
       "1014564 -0.384323  -0.717970            -0.010380 -1.457264 -1.445531     0   \n",
       "1014565 -0.540124  -0.707209            -0.010380 -1.457264 -1.559327     0   \n",
       "1014566 -1.500124  -1.363620            -0.010380 -1.457264 -1.673123     0   \n",
       "\n",
       "        SchoolHoliday Promo2  StateHoliday_a  StateHoliday_b  StateHoliday_c  \\\n",
       "0                   1      0               0               0               0   \n",
       "1                   1      0               0               0               0   \n",
       "2                   1      0               0               0               0   \n",
       "3                   1      0               0               0               0   \n",
       "4                   1      0               0               0               0   \n",
       "...               ...    ...             ...             ...             ...   \n",
       "1014562             1      1               0               0               0   \n",
       "1014563             1      1               0               0               0   \n",
       "1014564             1      1               0               0               0   \n",
       "1014565             1      1               0               0               0   \n",
       "1014566             1      1               1               0               0   \n",
       "\n",
       "         StoreType_b  StoreType_c  StoreType_d  Assortment_b  Assortment_c  \\\n",
       "0                  0            1            0             0             0   \n",
       "1                  0            1            0             0             0   \n",
       "2                  0            1            0             0             0   \n",
       "3                  0            1            0             0             0   \n",
       "4                  0            1            0             0             0   \n",
       "...              ...          ...          ...           ...           ...   \n",
       "1014562            0            0            1             0             1   \n",
       "1014563            0            0            1             0             1   \n",
       "1014564            0            0            1             0             1   \n",
       "1014565            0            0            1             0             1   \n",
       "1014566            0            0            1             0             1   \n",
       "\n",
       "         DayOfWeek_1  DayOfWeek_2  DayOfWeek_3  DayOfWeek_5  DayOfWeek_6  \n",
       "0                  0            0            0            1            0  \n",
       "1                  0            0            0            0            0  \n",
       "2                  0            0            1            0            0  \n",
       "3                  0            1            0            0            0  \n",
       "4                  1            0            0            0            0  \n",
       "...              ...          ...          ...          ...          ...  \n",
       "1014562            0            0            0            0            1  \n",
       "1014563            0            0            0            1            0  \n",
       "1014564            0            0            0            0            0  \n",
       "1014565            0            0            1            0            0  \n",
       "1014566            0            1            0            0            0  \n",
       "\n",
       "[1014567 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Sales'\n",
    "features_cols = data.columns.drop(target_col)        \n",
    "features = data[features_cols]\n",
    "target = data[target_col]\n",
    "target = pd.DataFrame(target.values.reshape(-1,1))\n",
    "features = features.values\n",
    "target = target.values\n",
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.4, shuffle=False)\n",
    "x_test,x_valid,y_test,y_valid = train_test_split(x_test, y_test, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "y_valid = y_valid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data의 row : 608740, column : 20\n",
      "test_data의 row : 202913, column : 20\n",
      "val_data의 row : 202914, column : 20\n"
     ]
    }
   ],
   "source": [
    "print('train_data의 row : %s, column : %s'%(x_train.shape[0],x_train.shape[1]))\n",
    "print('test_data의 row : %s, column : %s'%(x_test.shape[0],x_test.shape[1]))\n",
    "print('val_data의 row : %s, column : %s'%(x_valid.shape[0],x_valid.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               5376      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 173,569\n",
      "Trainable params: 171,777\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(256,input_dim = x_train.shape[1],activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256,activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256,activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128,activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='mean_squared_error',\n",
    "             metrics=['mean_absolute_error'])\n",
    "\n",
    "model_path = 'C:/project/rossmann-store-sales'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=model_path , monitor='val_mean_absolute_error', verbose=1, save_best_only=True)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9499/9512 [============================>.] - ETA: 0s - loss: 0.1260 - mean_absolute_error: 0.2539\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.23139, saving model to C:/project\\rossmann-store-sales\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:/project\\rossmann-store-sales\\assets\n",
      "9512/9512 [==============================] - 87s 9ms/step - loss: 0.1259 - mean_absolute_error: 0.2539 - val_loss: 0.1280 - val_mean_absolute_error: 0.2314\n",
      "Epoch 2/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0904 - mean_absolute_error: 0.2217\n",
      "Epoch 00002: val_mean_absolute_error improved from 0.23139 to 0.20416, saving model to C:/project\\rossmann-store-sales\n",
      "INFO:tensorflow:Assets written to: C:/project\\rossmann-store-sales\\assets\n",
      "9512/9512 [==============================] - 91s 10ms/step - loss: 0.0904 - mean_absolute_error: 0.2216 - val_loss: 0.2085 - val_mean_absolute_error: 0.2042\n",
      "Epoch 3/50\n",
      "9505/9512 [============================>.] - ETA: 0s - loss: 0.0864 - mean_absolute_error: 0.2164\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 0.20416\n",
      "9512/9512 [==============================] - 89s 9ms/step - loss: 0.0864 - mean_absolute_error: 0.2164 - val_loss: 0.1725 - val_mean_absolute_error: 0.2109\n",
      "Epoch 4/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0838 - mean_absolute_error: 0.2133\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 0.20416\n",
      "9512/9512 [==============================] - 74s 8ms/step - loss: 0.0838 - mean_absolute_error: 0.2133 - val_loss: 0.2153 - val_mean_absolute_error: 0.2195\n",
      "Epoch 5/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0813 - mean_absolute_error: 0.2101\n",
      "Epoch 00005: val_mean_absolute_error improved from 0.20416 to 0.20179, saving model to C:/project\\rossmann-store-sales\n",
      "INFO:tensorflow:Assets written to: C:/project\\rossmann-store-sales\\assets\n",
      "9512/9512 [==============================] - 101s 11ms/step - loss: 0.0813 - mean_absolute_error: 0.2101 - val_loss: 0.1754 - val_mean_absolute_error: 0.2018\n",
      "Epoch 6/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0803 - mean_absolute_error: 0.2092- \n",
      "Epoch 00006: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 94s 10ms/step - loss: 0.0803 - mean_absolute_error: 0.2092 - val_loss: 0.2735 - val_mean_absolute_error: 0.2084\n",
      "Epoch 7/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0801 - mean_absolute_error: 0.2090\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 92s 10ms/step - loss: 0.0801 - mean_absolute_error: 0.2090 - val_loss: 0.1356 - val_mean_absolute_error: 0.2074\n",
      "Epoch 8/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0788 - mean_absolute_error: 0.2076\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 93s 10ms/step - loss: 0.0788 - mean_absolute_error: 0.2076 - val_loss: 0.2550 - val_mean_absolute_error: 0.2245\n",
      "Epoch 9/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0783 - mean_absolute_error: 0.2072\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 96s 10ms/step - loss: 0.0783 - mean_absolute_error: 0.2072 - val_loss: 0.1984 - val_mean_absolute_error: 0.2076\n",
      "Epoch 10/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0776 - mean_absolute_error: 0.2063\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 86s 9ms/step - loss: 0.0776 - mean_absolute_error: 0.2063 - val_loss: 0.2247 - val_mean_absolute_error: 0.2173\n",
      "Epoch 11/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0766 - mean_absolute_error: 0.2051\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 81s 9ms/step - loss: 0.0766 - mean_absolute_error: 0.2051 - val_loss: 0.1340 - val_mean_absolute_error: 0.2101\n",
      "Epoch 12/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0767 - mean_absolute_error: 0.2050\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 90s 9ms/step - loss: 0.0767 - mean_absolute_error: 0.2050 - val_loss: 0.2704 - val_mean_absolute_error: 0.2044\n",
      "Epoch 13/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0765 - mean_absolute_error: 0.2050\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 94s 10ms/step - loss: 0.0765 - mean_absolute_error: 0.2050 - val_loss: 0.2946 - val_mean_absolute_error: 0.2131\n",
      "Epoch 14/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0759 - mean_absolute_error: 0.2039\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 91s 10ms/step - loss: 0.0759 - mean_absolute_error: 0.2039 - val_loss: 0.2800 - val_mean_absolute_error: 0.2289\n",
      "Epoch 15/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0757 - mean_absolute_error: 0.2040\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 92s 10ms/step - loss: 0.0757 - mean_absolute_error: 0.2040 - val_loss: 0.2716 - val_mean_absolute_error: 0.2094\n",
      "Epoch 16/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0761 - mean_absolute_error: 0.2048\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 89s 9ms/step - loss: 0.0761 - mean_absolute_error: 0.2048 - val_loss: 0.2668 - val_mean_absolute_error: 0.2267\n",
      "Epoch 17/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0751 - mean_absolute_error: 0.2033\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 81s 8ms/step - loss: 0.0751 - mean_absolute_error: 0.2033 - val_loss: 0.3323 - val_mean_absolute_error: 0.2232\n",
      "Epoch 18/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0751 - mean_absolute_error: 0.2033\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 88s 9ms/step - loss: 0.0751 - mean_absolute_error: 0.2033 - val_loss: 0.2296 - val_mean_absolute_error: 0.2041\n",
      "Epoch 19/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0745 - mean_absolute_error: 0.2028\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 93s 10ms/step - loss: 0.0745 - mean_absolute_error: 0.2028 - val_loss: 0.2147 - val_mean_absolute_error: 0.2223\n",
      "Epoch 20/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0742 - mean_absolute_error: 0.2025\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 94s 10ms/step - loss: 0.0742 - mean_absolute_error: 0.2025 - val_loss: 0.2186 - val_mean_absolute_error: 0.2028\n",
      "Epoch 21/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0741 - mean_absolute_error: 0.2022\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 95s 10ms/step - loss: 0.0741 - mean_absolute_error: 0.2022 - val_loss: 0.2402 - val_mean_absolute_error: 0.2115\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0743 - mean_absolute_error: 0.2026\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 90s 9ms/step - loss: 0.0743 - mean_absolute_error: 0.2026 - val_loss: 0.2272 - val_mean_absolute_error: 0.2139\n",
      "Epoch 23/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0733 - mean_absolute_error: 0.2007\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 83s 9ms/step - loss: 0.0733 - mean_absolute_error: 0.2007 - val_loss: 0.2502 - val_mean_absolute_error: 0.2050\n",
      "Epoch 24/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0740 - mean_absolute_error: 0.2020\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 87s 9ms/step - loss: 0.0740 - mean_absolute_error: 0.2020 - val_loss: 0.3304 - val_mean_absolute_error: 0.2251\n",
      "Epoch 25/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0733 - mean_absolute_error: 0.2009\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 101s 11ms/step - loss: 0.0733 - mean_absolute_error: 0.2009 - val_loss: 0.2340 - val_mean_absolute_error: 0.2035\n",
      "Epoch 26/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0731 - mean_absolute_error: 0.2007\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 102s 11ms/step - loss: 0.0731 - mean_absolute_error: 0.2007 - val_loss: 0.2754 - val_mean_absolute_error: 0.2093\n",
      "Epoch 27/50\n",
      "9504/9512 [============================>.] - ETA: 0s - loss: 0.0731 - mean_absolute_error: 0.2009\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 87s 9ms/step - loss: 0.0731 - mean_absolute_error: 0.2009 - val_loss: 0.3174 - val_mean_absolute_error: 0.2053\n",
      "Epoch 28/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0732 - mean_absolute_error: 0.2012\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 93s 10ms/step - loss: 0.0732 - mean_absolute_error: 0.2012 - val_loss: 0.2334 - val_mean_absolute_error: 0.2062\n",
      "Epoch 29/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0732 - mean_absolute_error: 0.2010\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 95s 10ms/step - loss: 0.0732 - mean_absolute_error: 0.2010 - val_loss: 0.4602 - val_mean_absolute_error: 0.2180\n",
      "Epoch 30/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0731 - mean_absolute_error: 0.2009\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 87s 9ms/step - loss: 0.0731 - mean_absolute_error: 0.2009 - val_loss: 0.5633 - val_mean_absolute_error: 0.2213\n",
      "Epoch 31/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0727 - mean_absolute_error: 0.2003\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 89s 9ms/step - loss: 0.0727 - mean_absolute_error: 0.2003 - val_loss: 0.2458 - val_mean_absolute_error: 0.2108\n",
      "Epoch 32/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0726 - mean_absolute_error: 0.2003\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 97s 10ms/step - loss: 0.0726 - mean_absolute_error: 0.2003 - val_loss: 0.3909 - val_mean_absolute_error: 0.2319\n",
      "Epoch 33/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0718 - mean_absolute_error: 0.1991\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 88s 9ms/step - loss: 0.0718 - mean_absolute_error: 0.1991 - val_loss: 0.3660 - val_mean_absolute_error: 0.2135\n",
      "Epoch 34/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0721 - mean_absolute_error: 0.1998- ETA: 2s - lo\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 76s 8ms/step - loss: 0.0721 - mean_absolute_error: 0.1998 - val_loss: 0.5260 - val_mean_absolute_error: 0.2211\n",
      "Epoch 35/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0718 - mean_absolute_error: 0.1989\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 98s 10ms/step - loss: 0.0718 - mean_absolute_error: 0.1989 - val_loss: 0.4444 - val_mean_absolute_error: 0.2146\n",
      "Epoch 36/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0720 - mean_absolute_error: 0.1996\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 99s 10ms/step - loss: 0.0720 - mean_absolute_error: 0.1996 - val_loss: 0.3357 - val_mean_absolute_error: 0.2135\n",
      "Epoch 37/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0719 - mean_absolute_error: 0.1993\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 100s 11ms/step - loss: 0.0719 - mean_absolute_error: 0.1993 - val_loss: 0.3826 - val_mean_absolute_error: 0.2182\n",
      "Epoch 38/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0716 - mean_absolute_error: 0.1989\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 100s 10ms/step - loss: 0.0716 - mean_absolute_error: 0.1989 - val_loss: 0.3529 - val_mean_absolute_error: 0.2105\n",
      "Epoch 39/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0718 - mean_absolute_error: 0.1992\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 100s 11ms/step - loss: 0.0718 - mean_absolute_error: 0.1992 - val_loss: 0.4715 - val_mean_absolute_error: 0.2171\n",
      "Epoch 40/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0715 - mean_absolute_error: 0.1992\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 103s 11ms/step - loss: 0.0715 - mean_absolute_error: 0.1992 - val_loss: 0.5261 - val_mean_absolute_error: 0.2233\n",
      "Epoch 41/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0715 - mean_absolute_error: 0.1989\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 99s 10ms/step - loss: 0.0715 - mean_absolute_error: 0.1989 - val_loss: 0.4796 - val_mean_absolute_error: 0.2220\n",
      "Epoch 42/50\n",
      "9512/9512 [==============================] - ETA: 0s - loss: 0.0710 - mean_absolute_error: 0.1982- ETA: 1s - loss: 0.0710 - \n",
      "Epoch 00042: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 99s 10ms/step - loss: 0.0710 - mean_absolute_error: 0.1982 - val_loss: 0.3590 - val_mean_absolute_error: 0.2149\n",
      "Epoch 43/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0712 - mean_absolute_error: 0.1982\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 96s 10ms/step - loss: 0.0712 - mean_absolute_error: 0.1982 - val_loss: 0.3997 - val_mean_absolute_error: 0.2283\n",
      "Epoch 44/50\n",
      "9509/9512 [============================>.] - ETA: 0s - loss: 0.0714 - mean_absolute_error: 0.1987\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 98s 10ms/step - loss: 0.0714 - mean_absolute_error: 0.1987 - val_loss: 0.4268 - val_mean_absolute_error: 0.2137\n",
      "Epoch 45/50\n",
      "9511/9512 [============================>.] - ETA: 0s - loss: 0.0710 - mean_absolute_error: 0.1984\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 101s 11ms/step - loss: 0.0710 - mean_absolute_error: 0.1984 - val_loss: 0.4970 - val_mean_absolute_error: 0.2351\n",
      "Epoch 46/50\n",
      "9510/9512 [============================>.] - ETA: 0s - loss: 0.0711 - mean_absolute_error: 0.1983\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 100s 11ms/step - loss: 0.0711 - mean_absolute_error: 0.1983 - val_loss: 0.4242 - val_mean_absolute_error: 0.2374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "9508/9512 [============================>.] - ETA: 0s - loss: 0.0704 - mean_absolute_error: 0.1973\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 98s 10ms/step - loss: 0.0704 - mean_absolute_error: 0.1973 - val_loss: 0.4182 - val_mean_absolute_error: 0.2143\n",
      "Epoch 48/50\n",
      "9505/9512 [============================>.] - ETA: 0s - loss: 0.0711 - mean_absolute_error: 0.1983\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 79s 8ms/step - loss: 0.0711 - mean_absolute_error: 0.1983 - val_loss: 0.5598 - val_mean_absolute_error: 0.2268\n",
      "Epoch 49/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0704 - mean_absolute_error: 0.1976\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 81s 9ms/step - loss: 0.0704 - mean_absolute_error: 0.1976 - val_loss: 0.4765 - val_mean_absolute_error: 0.2226\n",
      "Epoch 50/50\n",
      "9507/9512 [============================>.] - ETA: 0s - loss: 0.0704 - mean_absolute_error: 0.1976\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 0.20179\n",
      "9512/9512 [==============================] - 88s 9ms/step - loss: 0.0704 - mean_absolute_error: 0.1975 - val_loss: 0.5090 - val_mean_absolute_error: 0.2229\n",
      "처리시간 :  1:16:48.006067\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "results= model.fit(x_train, y_train,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "                   callbacks=[checkpoint])\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "time = end-start\n",
    "print(\"처리시간 : \", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 : 테스트 데이터로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_365 = pred[0:202913]\n",
    "y_test_365 = y_valid[0:202913]\n",
    "\n",
    "fig = plt.figure(facecolor='white')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(y_test_365, label='True')\n",
    "ax.plot(pred_365, label='Prediction')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6714616293555522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(pred_365, y_test_365)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71345836\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(pred, y_valid, squared=False)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
